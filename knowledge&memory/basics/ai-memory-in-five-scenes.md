# AI 记忆（Memory）的五个场景——从通用大语言模型（GPT）到图谱检索增强（GraphRAG）的卓越之路

> **核心总结**：通过五个生活阶段的假设场景，从实用的角度分析 AI 记忆（Memory）框架的演进：从基础大语言模型（Base LLM）到经典检索增强生成（RAG）再到先进的、图感知的检索增强生成（Graph-aware RAG）。本文聚焦于基于知识图谱（Knowledge Graph）的图谱检索增强（GraphRAG）系统。核心洞察：简单检索增强生成（RAG）不足够，知识图谱（Knowledge Graph）通过结构化逻辑提供真正有用的上下文理解。

**发布日期：** 2025 年 8 月 20 日
**分类：** AI 架构 / AI 记忆（Memory）基础
**阅读时间：** 10 分钟
**作者：** Vasilije Markovic（Cognee 联合创始人/CEO）
**来源：** [Cognee Blog](https://www.cognee.ai/blog/fundamentals/ai-memory-in-five-scenes)

---

## 引言

告诉聚会上的你从事 AI 记忆（Memory）工作，观察他们的表情。典型的反应是礼貌的"哦，很酷"，伴随着困惑的内心齿轮默默转动。随意的 AI 用户通常会说类似"ChatGPT 已经可以记住聊天的开头——所以还有什么可工作的吗？"与此同时，领域内的人通常会选择尖锐的反驳，比如"所以……嵌入（Embeddings）和检索增强生成（RAG），嗯？那对你来说效果如何？"

这在 cognee 工作的所有人身上都会发生。事实证明，即使是经验丰富的开发者，"AI 记忆（Memory）"可能意味着很多东西，它们之间的界限很快就会变得模糊。因此，我们想给读者提供五个不同生活阶段和情况下的假设场景的概念之旅。通过这些故事，我们将通过其实用的棱镜分析 AI 记忆（Memory）的框架演进：从**基础大语言模型（Base LLM）**到**经典检索增强生成（Classic RAG）**再到**先进的、图感知的检索增强生成（Advanced Graph-aware RAG）**。

为了保持主题聚焦，我们将仅涵盖基于知识图谱（Knowledge Graph）的、图谱检索增强（GraphRAG）风格的系统，并将智能体（Agent）/长视野（Long-horizon）的讨论留待另一天。

已经困惑了？让我们降低一点难度，从场景 1 开始。

---

## 场景 1：图书馆里的孩子

想象自己是一个 12 岁的孩子在图书馆。是的，一个真正的图书馆，有真正的书，那种真正的旧书气味。你需要帮助找到一本书，帮助你为特定的学校项目学习恐龙知识。在这个图书馆里，有三名员工可用。

首先，**新来的年轻员工**。他确实博览群书，如果你问他，他可以自信地谈论恐龙——但他对这个特定图书馆一无所知。问他恐龙书在哪里，他只会眨眼。知识丰富；没有空间感。

接下来，你遇到了**盯着屏幕的员工**。他可以输入任何书名并准确找到它的位置，然后为你阅读它的简介。准确吗？是的。有用吗？有点。他知道这本书好不好，或者它能否帮助你完成作业吗？一点线索都没有。他全是搜索，没有意义。

最后，有**高级图书管理员**。她**就是**图书馆。她不仅仅是找到书；她生活在其中，呼吸着它们，知道它们如何连接。她记得一些旧的历史书，其中有你学校项目所需的恐龙章节。她不只是给你事实；她理解一切如何组合在一起。

这三名员工就像你今天可以使用的三种类型的 AI。你可能使用过 ChatGPT，它是一种称为**大语言模型（Large Language Model, LLM）**的 AI 类型。新来的年轻员工就像基础大语言模型（Base LLM）——广博、善辩，但不知道你的特定需求。

目录阅读者是**基础检索增强生成（Basic RAG）**：擅长查找和引用事实，没有更深的结构。而高级图书管理员是**图感知检索（Graph-aware Retrieval）**：它知道片段如何相关，所以它可以真正帮助你解决任务，而不仅仅是交给你文本。我们将在下一级探索这两种方法是如何工作的。

---

## 场景 2：看电影的高中生

你正在尝试在流媒体服务上选择一部电影——你想看一部很棒的动画电影，你有三个 AI 选项。

首先是：**ChatGPT**。它建议《千与千寻》或《玩具总动员》。绝对是很好的选择——但它不知道你订阅的平台上实际有什么可用。那就是基础大语言模型（Base LLM）：通常很聪明，但与现实脱节。

下一个选项：**平台的免费机器人**。它搜索平台的数据库并检索顶级结果。然后它将一大块摘要、演员阵容和评论铲入大语言模型（LLM）以及你的问题旁边。这个组合的文本是模型看到的**上下文（Context）**。总比没有好——里面有真实数据——但它嘈杂、冗余，对你真正关心的事情漠不关心。

第三个选项：**个人电影大师**。这个 AI 理解你。它搜索相同的目录，但不是倾倒原始文本，而是根据它对你的了解**构建**一个更清晰的上下文："用户偏好皮克斯 + 冒险。《飞屋环游记》两者都匹配；《寻梦环游记》在基调上相似；两者都可在他们的计划中使用。"它可能会调用大语言模型（LLM）几次来完善那个脚手架。结果？结构化、偏好感知、基于可用性的。

---

## 场景 3：准备考试的大学生

你即将参加考试，你想使用 AI 帮助你学习。你的讲座笔记、幻灯片和扫描章节散布在笔记本电脑的文件夹中。

如果你问 ChatGPT **Miller 教授**关于熵说了什么——它将提供该概念的不错定义，但不是来自你的实际课程。你可以逐个传递文档，但这很乏味，而且模型无论如何都无法在记忆中保存所有内容。

但是有一些工具实际上可以帮助。这些工具首先将你的笔记分解成小的**块（Chunks）**。每个块被转换成一个捕获文本含义的数字序列——**向量嵌入（Vector Embedding）**。

你关于 Miller 教授对熵的评论的问题也被向量化，系统将通过**语义搜索（Semantic Search）**找到最相似的块。然后这些块被附加到你的提示并发送给大语言模型（LLM）。

这就是**检索增强生成（Retrieval-Augmented Generation, RAG）**的实际应用。对于像你这样的直接的、事实性问题，它可以很好地工作：快速、相关、准确。

但是一个可以理解的后续问题，比如"教授为熵使用了哪些例子**不**在教科书中？"需要**跨文档推理（Cross-document Reasoning）**，应用**负过滤器（Negative Filter）**。标准检索增强生成（RAG）在这里会挣扎，因为仅相似度搜索不捕获诸如"讲座示例"与"教科书示例"之类的关系，也不跟踪不同来源之间的引用。

更先进的系统所做的是开始**命名**你材料中的东西——'示例'、'理论'、'公式'——并连接它们：讲座 → 提及 → 示例；教科书 → 包含 → 示例。它们仍然保留文本块，但也存储**实体（Entities）**和它们的**关系（Relationships）**。现在我们正在接近理解思想如何相关的**知识图谱（Knowledge Map）**，而不仅仅是哪些段落看起来相似。在场景 4 中保留这个想法。

---

## 场景 4：找工作的初级 IT 工程师

你正在找工作，想要缩小选择范围，最好是具有 20-50 名员工的 B 轮初创公司的远程优先职位，使用 Python 和 React，并且不在金融科技行业。

问 ChatGPT-with-web，你会得到一个通用列表——合理的名字，验证薄弱，很少承认你的约束。

然后你爬取不同平台的招聘职位并将它们通过管道传输到**检索增强生成（RAG）**。这比没有好——它浮现出**提及**你关键字的职位。但它仍然很难**强制执行**完整的过滤器集。"初创公司文化"潜入后期公司；"远程"出现在"混合角色：50/50 办公室和远程"；否则完美的职位结果是金融科技。

你可以做的是：使用大语言模型（LLM）调用**提取实体（Extract Entities）**——比如**公司**、**职位**、**技能**、**融资阶段（FundingStage）**、**行业**、**位置策略（LocationPolicy）**、**团队规模（TeamSize）**——和**关系（Relations）**，如**雇佣（employs）**、**需要（requires）**、**运营于（operates_in）**、**是阶段（is_stage）**、**有规模（has_size）**。将这些存储为链接回证据段落的**知识图谱（Knowledge Graph）**。现在检索不是"给我相似的文本"，而是"给我满足（阶段 = B 轮）∧（团队规模 ∈ [20,50]）∧（技能 ⊇ {Python, React}）∧（行业 ≠ 金融科技）∧（远程优先 = true）的**子图（Subgraph）**，加上引用。"这是**图谱检索增强（GraphRAG）**：实体/关系和支持片段的混合搜索。

你可以嵌入实体（Entities）和边以保持语义相似度发挥作用，然后使用图过滤器精确地尊重约束。结果感觉再次像高级图书管理员——**结构化意义优先**，附带收据。

---

## 场景 5：派对上的 AI 初创公司 CTO

你回到那个派对。一个人点头并继续对话：一位经验丰富的专业人士，有人分享你对深度技术讨论的热情，有足够的故事填满一本书。你们对 AI 从笨拙的基于规则的系统到理论上令人印象深刻但在实践中不稳定的检索增强生成（RAG）模型的演变进行了极客式的讨论。

你们都同意，实际构建这种东西的人之间的共识是明确的：唯一的出路是**混合记忆（Hybrid Memory）**架构，将语义搜索建立在知识图谱（Knowledge Graph）的结构化逻辑中。在现实世界中，平衡速度、质量和可衡量性的经典挑战存在。

**速度。** 混合检索听起来很优雅，直到你在追求亚秒响应的同时结合向量相似度、倒排索引和子图过滤器。你可以拥有世界上最复杂的检索逻辑，但如果它需要三十秒返回结果，用户会弃船而逃。修复方法是务实的：预计算热子图；在检索和答案层激进缓存；使用轻量级分类器路由查询（检索增强生成（RAG）vs 图谱检索增强（GraphRAG））；当时间预算紧张时优雅降级（粗 → 精检索）。

**质量。** 结果取决于**有意义的实体类型（Entity Types）**和**模式**，这通常需要自定义本体。手动策划本体不可扩展，自动本体生成听起来很棒，直到你意识到关系需要多么特定于领域。所以你迭代：从实体/边的最小词汇开始；附加**时间属性（Temporal Attributes）**（有效期至/从），随着事实变化，添加基于事件的**知识图谱（Knowledge Graphs）**，突然，你发现自己处理数千个实体及其演变关系的版本控制而不破坏现有查询。

**可衡量性。** 精确度/召回率不再足够。你需要跟踪**检索质量（Retrieval Quality）**（覆盖范围、冗余、泄漏）、**答案忠实度（Answer Faithfulness）**（基础性、引用准确性）和**运营服务等级协议（Operational SLOs）**（延迟、吞吐量、成本/答案）的套件。大语言模型（LLM）辅助基准测试相对有用——当与稳定的"黄金"集和人工点检查配对时。如果没有严格的评估框架和系统行为的透明概览，你就是在用沙子而不是混凝土建造。

对话中有回荡的暂停——不是尴尬的，只是相互承认的。挑战是真实的，但上行也是如此。下一代 AI 系统将应对跨越许多专业领域的深层、多约束问题。

而且，如果你在 cognee 工作，很难不为朝着确切那个未来工作而感到自豪。

---

## AI 记忆（Memory）的下一步

我们已经从图书馆里困惑的孩子到经验丰富的 IT CTO；从基础大语言模型（Base LLM）到生产中的 AI 记忆（Memory）系统。你已经看到了为什么简单的检索增强生成（RAG）不够，以及知识图谱（Knowledge Graph）在哪里以及如何提供帮助。你已经阅读了幕后内容以及现实世界的挑战。

该领域发展很快，成为其中的一部分令人兴奋。还有太多要探索的：新想法和概念进入行业、AI 记忆（Memory）和智能体（Agent）系统之间复杂而丰富的连接、评估框架的演变……我们长期致力于这一切。

[加入我们的 Discord](https://discord.gg/cqF6RhDYWz)继续对话！如果你喜欢这种五级复杂性格式，请随时为未来的帖子建议主题！

---

*来源：Cognee Blog - Vasilije Markovic*
