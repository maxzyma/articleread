# RAG vs Memory：AI 智能体的知识架构差异

> **核心总结**：RAG 是"无状态的智能搜索引擎"，让 Agent **知道更多**；Memory 是"有状态的学习机制"，让 Agent **记住更好**——两者解决不同维度的上下文问题。文章提出 **Memory-First 架构**是未来方向，优先检索内部记忆再触发外部 RAG，可显著降低延迟和 API 成本。通过 11 个维度的架构对比，帮助开发者在企业知识库（RAG）、个性化体验（Memory）或混合模式中做出精准选择。

**发布日期：2025 年 10 月 7 日**
**分类：AI 架构**

**作者：Bobur Umurzokov - Developer Advocate**

---

## 简介

AI 智能体每天都在变得更强大。它们可以聊天、写代码、回答问题，并帮助完成曾经需要人类推理的任务。它们都面临一个共同的挑战：**如何随时间处理知识/上下文**。

为了解决这个问题，出现了两种架构模式：**检索增强生成（RAG）** 和 **Memory（记忆）**。两者的目标都是让大语言模型（LLMs）更强大、更有上下文意识、更具成本效益。但它们解决的问题不同，适用于智能体生命周期的不同阶段。

在本文中，我们将用简单的术语探讨这两种模式，展示它们的差异，并解释何时使用其中一种，或者何时结合使用两者。

---

## 问题：没有上下文的 LLM

LLM 在设计上是无状态的。每个提示都是独立处理的；一旦你发送新请求，模型就会忘记之前发生的所有事情，除非你再次将其包含在输入中。

这导致了三个核心限制：

1. **无持久性** - 模型不记得过去的会话或用户特定数据。
2. **高 Token 成本** - 为了"提醒"模型上下文，你必须不断追加很长的历史记录。
3. **有限的事实基础** - 如果信息不在训练集中，模型可能会产生幻觉或给出过时的答案。

---

## 什么是 RAG？

RAG 是构建在 LLM 周围的**检索层**。它不依赖模型的内部参数，而是在查询时动态注入外部知识。

该架构通常包含三个部分：

1. **索引管道** - 预处理文档并将其嵌入到向量数据库（例如 Pinecone、Weaviate、Qdrant、pgvector）中。
2. **检索管道** - 在每次查询时，将用户输入转换为嵌入并找到语义相似的文档。
3. **生成步骤** - 将查询与检索到的上下文结合，发送给 LLM 进行最终答案生成。

这个模式可以表示为：

```python
Answer = LLM(prompt + top_k(retrieve(query)))
```

### RAG 实际应用示例

以公司内部文档的 AI 助手为例。模型不知道你的私人文档，因为它们不是训练数据的一部分。使用 RAG，你可以：

- 将所有公司文档存储在向量数据库中（如 Pinecone、Weaviate 或 Qdrant）。
- 当用户问"如何重置密码？"时，助手从这些文档中检索相似文本。
- 模型然后读取检索到的文本并生成答案。

在这个设置中，**知识源是外部的**（例如文档语料库或数据库）且**无状态的**（每次查询都从头开始）。

### 为什么 RAG 变得流行

RAG 之所以强大，是因为它解决了 LLM 的两个大问题：

- **过时的知识** - 模型在几个月或几年前训练，不知道最新事实。使用 RAG，你可以随时检索新信息。
- **私有数据** - 你可以提供自己的文档而无需重新训练模型。

这就是为什么 RAG 成为企业 AI 系统和聊天机器人的标准方法。

### RAG 的局限性

然而，RAG 有明显的边界：

- **无持久性** - 它不从交互中学习；每个查询都是独立的。
- **有限的个性化** - 检索是基于文档的，而不是基于用户的。
- **嵌入中的噪声** - 语义相似性可能返回不相关或冗余的文本。
- **运营成本** - 向量数据库需要维护、调优和嵌入更新。

从用户体验角度来看，RAG 感觉像一个智能搜索引擎——信息丰富，但不个性化。

---

## AI 智能体中的 Memory 是什么？

Memory 指的是智能体可以跨交互**读取、写入和更新**的**持久上下文存储**。智能体不仅从外部源提取事实，它还记录学到的东西并稍后重用。Memory 不仅仅是缓存，它是智能体推理状态的一部分。

Memory 允许 AI 智能体：

- 回想以前的交互
- 从中学习
- 更新其知识
- 随时间保持行为一致

这不仅是检索，而是关于**体验**。

### 智能体中的 Memory 示例

想象你告诉你的 AI 助手：

> "我不喜欢咖啡。"

第二天，你问：

> "你能推荐一款早餐饮料吗？"

如果智能体回答"浓缩咖啡"，那它显然忘记了你说的话。但如果它回答：

> "也许茶或果汁——因为你不喜欢咖啡，"

那它就记住了。

这就是 Memory 所能实现的：跨多个对话或任务的连续性和上下文。

### Memory 的架构层级

典型的 Memory 通常包含几个层级：

| 层级 | 目的 | 典型存储 |
|------|------|----------|
| **短期记忆** | 保持最近的对话轮次或活动上下文 | 内存缓冲区 / 提示窗口 |
| **长期记忆** | 在单个会话之外持久化知识 | SQL DB、JSON 存储或向量 DB |
| **工作记忆** | 跟踪推理或规划中的中间步骤 | 进程内内存 / 便签本 |

每个层级在平衡准确性、上下文和性能方面都有不同的作用。

### Memory 的技术实现

Memory 可以通过多种方式实现：

- **向量 Memory** - 摘要或关键事实被嵌入并通过相似性检索（像 RAG 但用于个人上下文）。
- **键值存储** - 存储结构化条目，如 `{user_id: preferences}` 用于快速查找。
- **基于 SQL 的 Memory** - 像 [Memori](https://github.com/gibsonai/memori) 这样的系统将记忆视为带有时间戳、TTL 和谱系的关系数据。
- **图 Memory** - 表示实体和概念之间的关系（对推理有用）。

每种方法都有不同的优势：

- 向量 Memory 捕捉语义
- SQL Memory 提供结构和治理
- 图 Memory 支持推理
- 键值 Memory 简单快速

### Memory 的局限性

- **存储复杂性** - 管理和总结大量历史并非易事。
- **遗忘和衰减** - 系统必须决定保留或丢弃什么。
- **版本控制和冲突解决** - 更新事实而不产生重复或矛盾。
- **隐私和合规** - 持久数据必须加密、访问控制，并可根据请求删除。

换句话说：Memory 改善了用户体验，但引入了数据管理挑战。

---

## RAG vs Memory：架构比较

让我们用技术术语总结差异。

| 方面 | RAG | Memory |
|------|-----|--------|
| **目标** | 按需检索外部知识 | 随时间保留内部体验 |
| **源** | 文档语料库 / 外部数据 | 对话历史 / 智能体状态 |
| **有状态性** | 无状态 | 有状态 |
| **检索方法** | 嵌入相似性 | 结构化或上下文召回 |
| **更新机制** | 更新文档索引 | 写入 Memory 存储 |
| **常见存储** | 向量 DB（Pinecone、Qdrant 等） | SQL DB、KV 存储、混合 |
| **用例** | 问答、搜索、知识基础 | 个性化、推理、长期连续性 |

简单来说：

- **RAG 帮助你的智能体知道更多**。
- **Memory 帮助你的智能体记住更好**。

---

## 为什么仅靠 RAG 是不够的

许多生产 LLM 解决方案今天纯粹依赖 RAG。它适用于文档繁重的任务，但在长期运行或自适应上下文中会失败。

### 缺乏时间感知

RAG 检索文档但不进化。智能体不能说"上周你告诉我……"，除非你手动重新输入那个对话。

### 低效的上下文窗口

没有持久 Memory，开发者每次都必须发送完整的对话——昂贵且缓慢。

### 缺乏用户适应

RAG 可以通过用户 ID 个性化结果，但它不会从行为中适应。Memory 实现了"通过交互学习"。

---

## 为什么仅靠 Memory 也不够

Memory 存储体验，但可能缺乏外部事实基础。

例如：

- 销售助手可以记住你的客户和笔记
- 但它仍然需要**检索**最新的 CRM 记录或定价表

没有 RAG，Memory 驱动的智能体可能会变成**上下文感知但事实过时**。

因此，在现代架构中，**RAG 和 Memory 互补**。

---

## RAG + Memory：混合模式

混合方法结合了**检索**（用于事实）和 **Memory**（用于体验）。

在运行时，智能体管道如下所示：

```text
→ 从长期 Memory 检索（个人上下文）
→ 检索外部文档（RAG）
→ 合并上下文
→ 通过 LLM 生成响应
→ 将新知识写回 Memory
```

这个架构反映了人类的运作方式。我们回想个人体验，查找外部信息，然后行动。

### 从 RAG 到 Memory-First 架构

RAG 是智能检索的第一大步。但未来在于 **Memory-First 架构**，其中智能体从它已经知道的内容开始，只在必要时使用检索。

Memory-First 智能体工作流程可能如下所示：

1. 查询 Memory："我已经知道这个了吗？"
2. 如果缺失，触发 RAG 检索外部数据。
3. 合并结果。
4. 响应并存储摘要以供将来使用。

这显著降低了延迟和 API 成本，因为检索是有条件的，而不是恒定的。

---

## 结论

RAG 是一个突破。它让 AI 系统无需重新训练即可访问实时信息。

但这只是第一步。**Memory 扩展了这个基础**，使智能体能够跨会话学习、适应和个性化。

| 演化 | 重点 | 类比 |
|------|------|------|
| **RAG** | 信息检索 | 搜索引擎 |
| **Memory** | 持久学习 | 人类认知 |

下一代 AI 系统将结合两者——在需要时检索知识，同时保持连续的体验线程，使每次交互都比上一次更智能。

---

*来源：Gibson AI 博客，"RAG vs Memory for AI Agents: What's the Difference"，作者 Bobur Umurzokov*
