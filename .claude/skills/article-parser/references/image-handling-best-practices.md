# å›¾ç‰‡å¤„ç†æœ€ä½³å®è·µ

**æœ¬æ–‡æ¡£æ˜¯å›¾ç‰‡å¤„ç†çš„å”¯ä¸€çœŸç›¸æºï¼ˆSingle Source of Truthï¼‰**ã€‚

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜å›¾ç‰‡å¤„ç†çš„æ‰€æœ‰é«˜çº§æŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬ï¼šå¤–é“¾ç­–ç•¥ã€å‘½åè§„èŒƒã€ç¼“å­˜æ˜ å°„ã€éªŒè¯å·¥å…·ç­‰ã€‚

## ğŸ“‹ æ–‡æ¡£å®šä½

- **æœ¬æ–‡æ¡£**ï¼šå®Œæ•´çš„å›¾ç‰‡å¤„ç†æŒ‡å—ï¼ˆå”¯ä¸€çœŸç›¸æºï¼‰
- **SKILL.md**ï¼šå¿«é€Ÿå‚è€ƒå’Œæ‘˜è¦ï¼ˆæŒ‡å‘æœ¬æ–‡æ¡£ï¼‰

---

## æ ¸å¿ƒåŸåˆ™

1. **å¤–é“¾å‹å¥½å¹³å°ä¼˜å…ˆä½¿ç”¨åŸå§‹ URL**ï¼šTwitter/Xã€å¾®ä¿¡å…¬ä¼—å·ã€çŸ¥ä¹ç­‰å¹³å°æä¾›ç¨³å®š CDNï¼Œç›´æ¥ä½¿ç”¨åŸå§‹ URL å¯å‡å°‘æœ¬åœ°å­˜å‚¨éœ€æ±‚ã€‚

2. **âš ï¸ Standalone ç‰ˆæœ¬å¿…é¡»ä½¿ç”¨ base64 å†…åµŒå›¾ç‰‡**ï¼šæ— è®ºå¹³å°æ˜¯å¦æ”¯æŒå¤–é“¾ï¼Œstandalone ç‰ˆæœ¬çš„å›¾ç‰‡**å¿…é¡»**ä½¿ç”¨ base64 æ•°æ® URI åµŒå…¥ï¼Œç¡®ä¿æ–‡ä»¶å®Œå…¨ç‹¬ç«‹å¯ç§»æ¤ã€‚

**ä¸‰ä¸ªç‰ˆæœ¬çš„å›¾ç‰‡å¤„ç†è§„åˆ™**ï¼š
- **Original ç‰ˆæœ¬**ï¼šä½¿ç”¨ç›¸å¯¹è·¯å¾„æˆ–å¤–é“¾ URLï¼ˆæ ¹æ®å¹³å°ç±»å‹ï¼‰
- **Standalone ç‰ˆæœ¬**ï¼š**å¼ºåˆ¶ä½¿ç”¨ base64**ï¼Œä¸æ¥å—å¤–é“¾ï¼ˆè¿™æ˜¯å”¯ä¸€çœŸæ­£ç‹¬ç«‹çš„ç‰ˆæœ¬ï¼‰
- **Remote ç‰ˆæœ¬**ï¼šä½¿ç”¨å¤–é“¾ CDN URLï¼ˆä¾¿äºåœ¨çº¿åˆ†äº«ï¼‰

---

## ç¼“å­˜æ˜ å°„å…³ç³»ï¼ˆé˜²æ­¢å›¾ç‰‡é¡ºåºé”™ä½ï¼‰

### ç›®çš„

è®°å½•å›¾ç‰‡ä¸ä¸Šä¸‹æ–‡çš„ç²¾ç¡®å¯¹åº”å…³ç³»ï¼Œé˜²æ­¢å›¾ç‰‡é¡ºåºé”™ä½ã€‚

**æ ¸å¿ƒè§„åˆ™**ï¼šä½¿ç”¨ `context_before` å’Œ `context_after` ä½œä¸ºé”šç‚¹ï¼Œç²¾ç¡®å®šä½å›¾ç‰‡åœ¨æ–‡ç« ä¸­çš„ä½ç½®ã€‚

### ç¼“å­˜æ–‡ä»¶ç»“æ„

```
.claude/skills/article-parser/.cache/images/
â””â”€â”€ {article-slug}/
    â””â”€â”€ image-mapping.json
```

### æ˜ å°„æ–‡ä»¶æ ¼å¼

```json
{
  "article_url": "https://x.com/Khazix0918/status/2013812311388229792",
  "article_title": "Skillsçš„æœ€æ­£ç¡®ç”¨æ³•",
  "extraction_date": "2026-01-22",
  "images": [
    {
      "index": 1,
      "original_url": "https://pbs.twimg.com/media/G_J7mLHXsAA0gNV?format=png&name=900x900",
      "media_id": "G_J7mLHXsAA0gNV",
      "description": "ç¤ºä¾‹å›¾ç‰‡ - Googleæœç´¢",
      "local_filename": "google-search-mp3-to-wav.png",
      "alt_text": "ç¤ºä¾‹å›¾ç‰‡",
      "context_before": "skill-creatorï¼Œæ‰“åŒ…Githubä¸Šçš„å¼€æºé¡¹ç›®",
      "context_after": "å¯ä»¥å¿«é€Ÿåˆ›å»ºæŠ€èƒ½åŒ…"
    },
    {
      "index": 2,
      "original_url": "https://pbs.twimg.com/media/G_J8qXqaoAQ2xhu?format=jpg&name=large",
      "media_id": "G_J8qXqaoAQ2xhu",
      "description": "æ­£å‘åé¦ˆ",
      "local_filename": "G_J8qXqaoAQ2xhu.jpg",
      "alt_text": "æ­£å‘åé¦ˆ",
      "context_before": "ç”¨æˆ·åé¦ˆéå¸¸å¥½",
      "context_after": "æå‡äº†æ•ˆç‡"
    }
  ]
}
```

**å…³é”®å­—æ®µ**ï¼š
- `context_before`ï¼ˆå¿…éœ€ï¼‰ï¼šå›¾ç‰‡å‰çš„æ–‡å­—ç‰‡æ®µï¼Œä½œä¸ºå…³é”®é”šç‚¹
- `context_after`ï¼ˆå¯é€‰ï¼‰ï¼šå›¾ç‰‡åçš„æ–‡å­—ç‰‡æ®µï¼Œè¾…åŠ©éªŒè¯

### ç”Ÿæˆæ˜ å°„æ–‡ä»¶çš„è„šæœ¬

```python
import json
from datetime import datetime
from pathlib import Path

def create_image_mapping(article_url, images_data, article_dir):
    """åˆ›å»ºå›¾ç‰‡æ˜ å°„ç¼“å­˜æ–‡ä»¶

    Args:
        article_url: æ–‡ç« åŸå§‹ URL
        images_data: å›¾ç‰‡ä¿¡æ¯åˆ—è¡¨
            [{
                'url': 'åŸå§‹URL',
                'media_id': 'åª’ä½“IDï¼ˆå¦‚æœæœ‰ï¼‰',
                'description': 'å›¾ç‰‡æè¿°',
                'alt_text': 'Alt æ–‡æœ¬',
                'context_before': 'å›¾ç‰‡å‰çš„æ–‡å­—',
                'context_after': 'å›¾ç‰‡åçš„æ–‡å­—ï¼ˆå¯é€‰ï¼‰'
            }]
        article_dir: æ–‡ç« ç›®å½•è·¯å¾„
    """
    cache_dir = Path('.claude/skills/article-parser/.cache/images')
    cache_dir.mkdir(parents=True, exist_ok=True)

    # ä½¿ç”¨æ–‡ç«  slug ä½œä¸ºæ–‡ä»¶å
    article_slug = Path(article_dir).name
    mapping_file = cache_dir / f'{article_slug}.json'

    mapping = {
        'article_url': article_url,
        'extraction_date': datetime.now().isoformat(),
        'images': []
    }

    for i, img in enumerate(images_data, 1):
        # ä» URL æå– media_idï¼ˆTwitterï¼‰
        media_id = None
        if 'pbs.twimg.com' in img['url']:
            media_id = img['url'].split('/')[-1].split('?')[0]

        # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶åï¼ˆä½¿ç”¨ media_id æˆ–æè¿°æ€§åç§°ï¼‰
        if media_id:
            local_filename = f"{media_id}.jpg"
        else:
            desc = img['description'].lower().replace(' ', '-').replace('/', '-')
            local_filename = f"{desc}.jpg"

        mapping['images'].append({
            'index': i,
            'original_url': img['url'],
            'media_id': media_id,
            'description': img['description'],
            'local_filename': local_filename,
            'alt_text': img.get('alt_text', ''),
            'context_before': img.get('context_before', ''),
            'context_after': img.get('context_after', '')
        })

    with open(mapping_file, 'w', encoding='utf-8') as f:
        json.dump(mapping, f, ensure_ascii=False, indent=2)

    return mapping_file
```

---

## éªŒè¯å·¥å…·è„šæœ¬

### verify_images.py

éªŒè¯æ–‡ç« ä¸­çš„å›¾ç‰‡å¼•ç”¨æ˜¯å¦æ­£ç¡®ï¼ŒåŒ…æ‹¬ï¼š
- æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤çš„å›¾ç‰‡å¼•ç”¨
- éªŒè¯æ¯å¼ å›¾ç‰‡æ˜¯å¦åœ¨æ˜ å°„æ–‡ä»¶ä¸­
- æ ¹æ® context_before éªŒè¯å›¾ç‰‡ä½ç½®æ˜¯å¦æ­£ç¡®

```python
import re
import json
from pathlib import Path

def verify_images(markdown_file, mapping_file):
    """éªŒè¯å›¾ç‰‡å¼•ç”¨æ˜¯å¦æ­£ç¡®"""
    # è¯»å–æ˜ å°„æ–‡ä»¶
    with open(mapping_file) as f:
        mapping = json.load(f)

    # è¯»å– markdown æ–‡ä»¶
    with open(markdown_file) as f:
        content = f.read()

    # æå–æ‰€æœ‰å›¾ç‰‡å¼•ç”¨
    images_in_md = re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)

    # éªŒè¯
    issues = []

    # 1. æ£€æŸ¥é‡å¤å¼•ç”¨
    for i, (alt, url) in enumerate(images_in_md, 1):
        if images_in_md.count((alt, url)) > 1:
            issues.append(f"é‡å¤çš„å›¾ç‰‡: {alt} -> {url}")

    # 2. æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„ä¸­
    for alt, url in images_in_md:
        found = False
        for img in mapping['images']:
            if img['alt_text'] == alt or img['original_url'] == url:
                found = True
                break

        if not found:
            issues.append(f"æœªåœ¨æ˜ å°„ä¸­æ‰¾åˆ°: {alt} -> {url}")

    # 3. æ ¹æ® context_before éªŒè¯å›¾ç‰‡ä½ç½®
    for img in mapping['images']:
        if img.get('context_before'):
            # æŸ¥æ‰¾ context_before åœ¨æ–‡ç« ä¸­å‡ºç°çš„ä½ç½®
            before_pos = content.find(img['context_before'])
            if before_pos == -1:
                issues.append(f"æ‰¾ä¸åˆ°é”šç‚¹æ–‡å­—: {img['context_before']}")
                continue

            # æŸ¥æ‰¾å›¾ç‰‡å¼•ç”¨çš„ä½ç½®
            img_ref = f"![{img['alt_text']}]"
            img_pos = content.find(img_ref, before_pos)

            # éªŒè¯å›¾ç‰‡æ˜¯å¦ç´§è·Ÿåœ¨é”šç‚¹æ–‡å­—åé¢ï¼ˆåˆç†èŒƒå›´å†…ï¼‰
            if img_pos == -1 or img_pos - before_pos > 500:
                issues.append(
                    f"å›¾ç‰‡ä½ç½®å¯èƒ½ä¸å¯¹: {img['alt_text']} "
                    f"ï¼ˆåº”è¯¥åœ¨ '{img['context_before']}' é™„è¿‘ï¼‰"
                )

    if issues:
        print("âŒ å‘ç°é—®é¢˜ï¼š")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print("âœ… å›¾ç‰‡éªŒè¯é€šè¿‡")
        return True

# ä½¿ç”¨ç¤ºä¾‹
verify_images(
    'general/skills-github-toolbox/skills-github-toolbox.md',
    '.claude/skills/article-parser/.cache/images/skills-github-toolbox.json'
)
```

---

## ç”Ÿæˆ Standalone ç‰ˆæœ¬ï¼ˆBase64 å†…åµŒï¼‰

### âš ï¸ å…³é”®è§„åˆ™

**Standalone ç‰ˆæœ¬å¿…é¡»ä½¿ç”¨ base64 å†…åµŒå›¾ç‰‡ï¼Œæ— è®ºå¹³å°æ˜¯å¦æ”¯æŒå¤–é“¾ã€‚**

### Python å®ç°ç¤ºä¾‹

```python
import base64
import re
from pathlib import Path

def generate_standalone_version(markdown_file, output_file):
    """ç”Ÿæˆ standalone ç‰ˆæœ¬ï¼Œå°†æ‰€æœ‰å›¾ç‰‡è½¬æ¢ä¸º base64 å†…åµŒ

    Args:
        markdown_file: åŸå§‹ markdown æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡ºçš„ standalone æ–‡ä»¶è·¯å¾„
    """
    # è¯»å–åŸå§‹æ–‡ä»¶
    with open(markdown_file, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–æ‰€æœ‰å›¾ç‰‡ URL
    image_urls = re.findall(r'!\[([^\]]*)\]\(([^)]+)\)', content)

    # ä¸ºæ¯ä¸ªå›¾ç‰‡ä¸‹è½½å¹¶è½¬æ¢ä¸º base64
    for alt_text, url in image_urls:
        # è·³è¿‡å·²ç»æ˜¯ base64 çš„å›¾ç‰‡
        if url.startswith('data:'):
            continue

        try:
            # ä¸‹è½½å›¾ç‰‡
            import requests
            response = requests.get(url)
            response.raise_for_status()

            # è½¬æ¢ä¸º base64
            image_data = response.content
            # æ£€æµ‹å›¾ç‰‡ç±»å‹
            content_type = response.headers.get('content-type', 'image/png')
            base64_data = base64.b64encode(image_data).decode('utf-8')
            data_uri = f"data:{content_type};base64,{base64_data}"

            # æ›¿æ¢ URL ä¸º data URI
            old_ref = f"[{alt_text}]({url})"
            new_ref = f"[{alt_text}]({data_uri})"
            content = content.replace(old_ref, new_ref)

            print(f"âœ… è½¬æ¢å›¾ç‰‡: {alt_text}")

        except Exception as e:
            print(f"âŒ è½¬æ¢å¤±è´¥ {alt_text}: {e}")
            # ä¿ç•™åŸ URL
            continue

    # å†™å…¥ standalone ç‰ˆæœ¬
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(content)

    print(f"\nâœ… Standalone ç‰ˆæœ¬å·²ç”Ÿæˆ: {output_file}")
    print(f"æ–‡ä»¶å¤§å°: {Path(output_file).stat().st_size / 1024:.1f} KB")

# ä½¿ç”¨ç¤ºä¾‹
generate_standalone_version(
    'general/article/article.md',
    'general/article/article-standalone.md'
)
```

### æ‰¹é‡è½¬æ¢è„šæœ¬

```bash
#!/bin/bash
# convert_to_base64.sh

# ä¸ºæ¯ä¸ªå›¾ç‰‡ URL ä¸‹è½½å¹¶è½¬æ¢ä¸º base64
download_and_convert() {
    local url="$1"
    local output_file="$2"

    # ä¸‹è½½å›¾ç‰‡
    curl -s \
        -H "Referer: https://mp.weixin.qq.com/" \
        -H "User-Agent: Mozilla/5.0" \
        "$url" \
        -o "$output_file"

    # è½¬æ¢ä¸º base64
    base64_data=$(base64 -i "$output_file")
    echo "data:image/png;base64,$base64_data" > "${output_file}.b64"
}

# ç¤ºä¾‹ï¼šè½¬æ¢å¾®ä¿¡æ–‡ç« å›¾ç‰‡
imgIndex=1
download_and_convert \
    "https://mmbiz.qpic.cn/sz_mmbiz_png/...#imgIndex=${imgIndex}" \
    "image_${imgIndex}.png"
```

### éªŒè¯ Standalone ç‰ˆæœ¬

```python
def verify_standalone(markdown_file):
    """éªŒè¯ standalone ç‰ˆæœ¬æ˜¯å¦å®Œå…¨ç‹¬ç«‹

    æ£€æŸ¥ï¼š
    1. æ˜¯å¦æœ‰å¤–é“¾ URLï¼ˆåº”è¯¥ä¸º 0ï¼‰
    2. æ‰€æœ‰å›¾ç‰‡æ˜¯å¦ä¸º base64 æ ¼å¼
    3. æ–‡ä»¶å¤§å°åˆç†ï¼ˆbase64 ä¼šæ˜¾è‘—å¢å¤§ï¼‰
    """
    with open(markdown_file, 'r') as f:
        content = f.read()

    # æ£€æŸ¥å¤–é“¾
    external_links = re.findall(r'!\[([^\]]*)\]\((https?://[^)]+)\)', content)

    # æ£€æŸ¥ base64 å›¾ç‰‡
    base64_images = re.findall(r'!\[([^\]]*)\]\((data:[^)]+)\)', content)

    issues = []

    if external_links:
        issues.append(f"å‘ç° {len(external_links)} ä¸ªå¤–é“¾å›¾ç‰‡ï¼ˆstandalone ä¸åº”æœ‰å¤–é“¾ï¼‰")

    if not base64_images:
        issues.append("æœªæ‰¾åˆ° base64 å›¾ç‰‡")

    if issues:
        print("âŒ Standalone ç‰ˆæœ¬éªŒè¯å¤±è´¥ï¼š")
        for issue in issues:
            print(f"  - {issue}")
        return False
    else:
        print(f"âœ… Standalone ç‰ˆæœ¬éªŒè¯é€šè¿‡")
        print(f"  - Base64 å›¾ç‰‡: {len(base64_images)} ä¸ª")
        print(f"  - å¤–é“¾å›¾ç‰‡: 0 ä¸ª")
        print(f"  - æ–‡ä»¶å¤§å°: {Path(markdown_file).stat().st_size / 1024:.1f} KB")
        return True

# ä½¿ç”¨ç¤ºä¾‹
verify_standalone('general/article/article-standalone.md')
```

---

## æå–å·¥ä½œæµç¨‹ç¤ºä¾‹

### Twitter/X æ–‡ç« 

```python
# 1. æå–æ–‡ç« å†…å®¹å’Œå›¾ç‰‡
article_url = "https://x.com/Khazix0918/status/2013812311388229792"
content = extract_twitter_article(article_url)

# 2. æ”¶é›†å›¾ç‰‡ä¿¡æ¯ï¼ˆä¿ç•™åŸå§‹ URLï¼Œè®°å½•ä¸Šä¸‹æ–‡ï¼‰
images = []
for img in content.images:
    # æå–å›¾ç‰‡å‰åçš„æ–‡å­—ä½œä¸ºé”šç‚¹
    context_before = extract_text_before(img)
    context_after = extract_text_after(img)

    images.append({
        'url': img.url,  # ç›´æ¥ä½¿ç”¨åŸå§‹ URL
        'alt_text': img.alt_text,
        'description': extract_description_from_context(img),
        'context_before': context_before,
        'context_after': context_after
    })

# 3. åˆ›å»ºå›¾ç‰‡æ˜ å°„ç¼“å­˜
create_image_mapping(article_url, images, article_dir)

# 4. ç”Ÿæˆ markdownï¼ˆç›´æ¥ä½¿ç”¨åŸå§‹ URLï¼‰
markdown_content = generate_markdown(content, use_original_urls=True)

# 5. éªŒè¯å›¾ç‰‡é¡ºåº
verify_images(markdown_file, mapping_file)
```

### å°çº¢ä¹¦å›¾æ–‡ç¬”è®°

```python
# 1. æå–å›¾ç‰‡ URL
image_urls = extract_xiaohongshu_images(article_url)

# 2. ä¸‹è½½å›¾ç‰‡å¹¶ä½¿ç”¨æè¿°æ€§å‘½å
for i, url in enumerate(image_urls):
    # ä» OCR æˆ–ä¸Šä¸‹æ–‡ä¸­æå–æè¿°
    description = ocr_image(url)
    filename = f"{description.lower().replace(' ', '-')}.jpg"
    download_image(url, f"images/{filename}")

# 3. åˆ›å»ºç¼“å­˜æ˜ å°„ï¼ˆè®°å½•ä¸Šä¸‹æ–‡é”šç‚¹ï¼‰
image_data = extract_with_context(article_url)
create_image_mapping(article_url, image_data, article_dir)

# 4. ç”Ÿæˆ markdown
markdown_content = generate_markdown(content, image_dir='images')

# 5. éªŒè¯å›¾ç‰‡é¡ºåº
verify_images(markdown_file, mapping_file)
```

---

## æ€»ç»“

### æ ¸å¿ƒåŸåˆ™

1. **å¤–é“¾ä¼˜å…ˆ**ï¼šTwitter/Xã€å¾®ä¿¡å…¬ä¼—å·ç­‰å¹³å°ç›´æ¥ä½¿ç”¨åŸå§‹ URLï¼ˆoriginal å’Œ remote ç‰ˆæœ¬ï¼‰
2. **Standalone å¼ºåˆ¶ base64**ï¼šstandalone ç‰ˆæœ¬**å¿…é¡»**ä½¿ç”¨ base64 å†…åµŒï¼Œæ— è®ºå¹³å°æ˜¯å¦æ”¯æŒå¤–é“¾
3. **ä¸Šä¸‹æ–‡é”šç‚¹**ï¼šä½¿ç”¨ context_before ç²¾ç¡®å®šä½å›¾ç‰‡ä½ç½®ï¼Œé˜²æ­¢é”™ä½
4. **ç¼“å­˜æ˜ å°„**ï¼šåœ¨ `.cache/` è®°å½•å›¾ç‰‡æ˜ å°„å…³ç³»ï¼Œä¾¿äºè¿½æº¯å’ŒéªŒè¯
5. **å·¥å…·éªŒè¯**ï¼šä½¿ç”¨è„šæœ¬è‡ªåŠ¨åŒ–éªŒè¯ï¼Œé¿å…æ‰‹åŠ¨é”™è¯¯

### ä¸‰ä¸ªç‰ˆæœ¬çš„å›¾ç‰‡ç­–ç•¥

| ç‰ˆæœ¬ | å›¾ç‰‡æ¥æº | ä½¿ç”¨åœºæ™¯ | ç¤ºä¾‹ |
|------|---------|---------|------|
| **Original** | ç›¸å¯¹è·¯å¾„æˆ–å¤–é“¾ | æœ¬åœ°å½’æ¡£ï¼Œä¿ç•™çµæ´»æ€§ | `./images/image-1.png` æˆ–å¤–é“¾ URL |
| **Standalone** | **Base64 å†…åµŒ** | å®Œå…¨ç‹¬ç«‹çš„å•æ–‡ä»¶ï¼Œä¾¿äºåˆ†äº« | `data:image/png;base64,iVBORw0KGgo...` |
| **Remote** | CDN å¤–é“¾ | åœ¨çº¿åˆ†äº«ï¼Œå¿«é€ŸåŠ è½½ | `https://mmbiz.qpic.cn/...` |

**âš ï¸ å…³é”®è§„åˆ™**ï¼šStandalone ç‰ˆæœ¬**å¿…é¡»**ä½¿ç”¨ base64ï¼Œè¿™æ˜¯å”¯ä¸€ç¡®ä¿æ–‡ä»¶å®Œå…¨ç‹¬ç«‹å¯ç§»æ¤çš„æ–¹å¼ã€‚å³ä½¿å¹³å°æ”¯æŒå¤–é“¾ï¼ˆå¦‚å¾®ä¿¡ã€Twitterï¼‰ï¼Œstandalone ç‰ˆæœ¬ä¹Ÿä¸åº”ä½¿ç”¨å¤–é“¾ã€‚

### æ—§æ–¹å¼ vs æ–°æ–¹å¼

| ç»´åº¦ | æ—§æ–¹å¼ | æ–°æ–¹å¼ |
|------|--------|--------|
| å‘½å | `image-01.jpg` | `G_J8qXqaoAQ2xhu.jpg` æˆ– `user-comment.jpg` |
| å­˜å‚¨ | å¿…é¡»ä¸‹è½½ | å¤–é“¾å¹³å°ç›´æ¥ç”¨ URL |
| å®šä½ | é é¡ºåºå· | context_before é”šç‚¹ç²¾ç¡®å®šä½ |
| è¿½æº¯ | æ— è®°å½• | `.cache/` æ˜ å°„æ–‡ä»¶ |
| éªŒè¯ | äººå·¥æ£€æŸ¥ | è„šæœ¬è‡ªåŠ¨åŒ– |

---

### ç›¸å…³æ–‡æ¡£

- [validation-checklist.md](validation-checklist.md)ï¼šå®Œæ•´çš„éªŒè¯æ¸…å•
- [SKILL.md](../SKILL.md)ï¼šå¿«é€Ÿå‚è€ƒï¼ˆæœ¬æ–‡æ¡£çš„æ‘˜è¦ç‰ˆï¼‰
- [wechat-article-best-practices.md](wechat-article-best-practices.md)ï¼šå¾®ä¿¡å…¬ä¼—å·ç‰¹æ®Šå¤„ç†
